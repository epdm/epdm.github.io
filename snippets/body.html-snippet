
Definition of Quality
---------------------

**Quality** is a somewhat abstract, subjective quality with people
interpreting it differently. Like the 'theory of relativity' quality is
expressed as a relative concept and can be different things to different people.
For example: 


*"A Rolls Royce car is a quality car for certain customers whereas a VW Beatle
can be quality car for other customers."*


Therefore, consumers may tend to focus on the eventual specification quality of a product/service, or how it compares
with the competing companies in the marketplace. However, on the contrary,
producers might measure the conformance quality, or the degree to which the
product/service was produced correctly. The producer of a product may focus on
process variation minimisation, to achieve uniformity amongst and between
batches.


Five discrete and interrelated definitions of quality are listed below (Garvin,
1988):

* Transcendent (excellence);
* Product-based (amount of desireable attribute);
* User-based (fitness for use);
* Manufacturing-based (conformance to specification);
* Value-based (satisfaction relative to price);




Six Sigma
---------

Six sigma represents a set of techniques/tools for implementing process
improvement. This improvement is achieved through identifying and subsequently
removing the causes of defects, thus minimising variability in manufacturing and
business processes. A six sigma process is ome in wich 99.99966% of all
opportunities to produce some feature or part are statistically expected to be
free of defects (3.4 defective features per million opportunities). A defect in
this sense can be defined as any variation of a required characteristic of the
product for its parts, which is far enough removed from its nominal value to
precent the product from fulfilling the physical and functional requirements of
the customer.

Normal distribution curves are symmetrical about the mean value (μ), with the
total area under the curve equating to one. If a process is classified as 3
sigma, 99.7% of outputs are defect free. Mean is the central tendency of the
process, or the average of all values within the population. The standard
deviation (σ) is a measure of dispersion or variability.


![](images/Six Sigma.png "Six Sigma")


The upper specification limit (USL) and the lower specification limit (LSL) set
permissible limits for the process variation. For example, the USL and LSL of a
process may be set at 3 and -3, thus yielding a process which is 99.7% defect
free. USL represents a value designating an upper limit above which the process
or characteristic performance is unacceptable. In the converse case, the LSL
represents a value designating a lower limit, below which the process or
characteristic performance is unacceptable.

The relationship between yield, variability and specification limits is that
essentially the specification limits provide the permissible process variations.
Minimising variability therefore inherently leads to a better yield, as the
probability of defects occurring is substantially mitigated.  Cp and Cpk
represent metrics of process quality. Cp is a measure of how capable the process
is of producing the required process characteristic. Cp is known as the
capability index, or design margin, and is calculated using the following:


![](images/CpCalculation.png "Calculating Process Capability")


Cpk is a measure of actual performance which takes the actual mean into account.
Cpk is equal to Cp when the process mean is equal to the target, or nominal
value. Cpk essentially accounts for the process mean not hitting the target
value. The following calculations can be used to determine Cpk.


![](images/CpkCalculation.png "Calculating Cpk")


![](images/Cpk.png "Calculating Cpk")


![](images/KCalculation.png "Calculating K Value")


Poisson distribution describes the probability distribution of an event
occurring with respect to time, or space. The eventual process yield can be
calculated using Poisson's formula using the following, where dpu is the
measured defects per unit.


![](images/PoissonsDistribution.png "Poisson Distribution")


As an example if average dpu=1, the probability of having a device with no
defects can be calculated as:


![](images/Yield.png "Yield Calculation")



Relationship Between Quality and Reliability
--------------------------------------------

Quality assures the product will work after assembly, whereas reliability
provides the probability that the design will perform its intended function for
a designated period of time under specified conditions. Engineering process
reliability is the fundamental concept that is meant to anticipate quality
failures over the life cycle of a product. Variation of the process output may
affect both quality and reliability. 

Controlling reliability is much more
complex and cannot be controlled by a standard "quality" (Six Sigma) approach,
as they need a systems engineering approach. Quality is a snapshot at the start
of life and mainly related to control of lower level product specifications and
reliability is (as part of systems engineering) more of a system level motion
picture of the day-by-day operation for many years. Time zero defects are
manufacturing mistakes that escaped quality control, whereas, the additionald
effects that appear over time are 'reliability defects'.

Impact of Quality on Cycle Time and Cost
----------------------------------------

These three project managment constraints are traditionally represented in
what is known as the "Iron Triangle", depicted in the figure below.

![](images/iron_triangle.jpg "The Iron Triangle")

The relationship between the three constraints means that management staff
end up having to make a choice between two out of the three of them; in
other words, "Pick any two". This usually ends up with the three following
options:

* Designing a product to a high standard with a low time to market, at the cost of being expensive
* Designing a product quickly and at a low cost, at the expense of the product quality
* Designing a product to a high standard and at a low cost, but taking a long time to market.

ISO 9000
--------

The ISO 9000 is family of standards that gives requirements for an
organization's quality management system (QMS). It can be adopted to help ensure
organisations meet the standards of customers and stakeholders whilst meeting
the statutory and regulatory requirements of products. ISO 9000 is based on the
following eight quality management principles:

1. Customer focus
2. Leadership
3. Involvement of people
4. Process approach
5. System approach to management
6. Continual improvement
7. Factual approach to decision making
8. Mutually beneficial supplier relationships

Global adoption of the ISO 9000 standard is growing annually - the number of
global ISO certified organisations recorded in the world by the end of 2014 was
1,138,155, up from 409,421 by the end of 2000. The reason for the global adoption
can be attributed to a number of the following reasons:

1. Creates a more efficient, effective operation
2. Increases customer satisfaction and retention
3. Reduces audits
4. Enhances marketing
5. Improves employee motivation, awareness, and morale
6. Promotes international trade
7. Increases profit
8. Reduces waste and increases productivity
9. Common tool for standardization

A range of Case Studies (ISO 9000 Case Studies, 2016) can be viewed to support the
claims made.

Check Sheets
------------

A check sheet is a form used to collect data in real time at the location
where the data is generated. A simple example of a check sheet is presented below.

![](images/CheckSheet1.png "Simple Check Sheet")

The pertinent advantages of utilising check sheets for gathering data are as follows:

1. They are a quick, very easy and efficient means for recording desired information.
2. Information gathered can be either qualitative or quantitative.

Check sheets are generally be used to:

1. Quantify defects by type, location, and cause (e.g. from a machine or worker)
2. Keep track of the completion of steps in a multi-step procedure (i.e. be used as a checklist)
3. Check the shape of the probability distribution of a process.

Pareto
------

The Pareto principle (also known as the 80 - 20 rule, the law of the vital few,
and the principle of factor sparsity) states that, for many events, roughly 80%
of the effects come from 20% of the causes. The term 80 - 20 is only a shorthand
for the general principle at work. In individual cases, the distribution could
just as well be, say, 80 - 10 or 80 -30. There is no need for the two numbers to
add up to the number 100, as they are measures of different things. A simple
example of a pareto chart is presented below.

![](images/pareto_chart.png "Pareto Chart")

Adding up to 100 leads to a nice symmetry. For example, if 80% of effects come
from the top 20% of sources, then the remaining 20% of effects come from the
lower 80% of sources. This is called the "joint ratio", and can be used to
measure the degree of imbalance: a joint ratio of 96:4 is very imbalanced, 80:20
is significantly imbalanced, 70:30 is moderately imbalanced (Gini index: 40%), and
55:45 is just slightly imbalanced.

This principle is particularly prominent in the field of Software Engineering,
as highlighted by the following examples:

Microsoft noted that by fixing the top 20% of the most-reported bugs, 80% of the
related errors and crashes in a given system would be eliminated.

In load testing, it is common practice to estimate that 80% of the traffic occurs
during 20% of the time.

In Software Engineering, Lowell Arthur expressed a corollary principle: "20 percent
of the code has 80 percent of the errors. Find them, fix them!"

Fishbone Ishikawa Diagrams
--------------------------

Ishikawa diagrams (also called fishbone diagrams) are causal diagrams that show
the causes of a specific event. Common uses of the Ishikawa diagram are product
design and quality defect prevention to identify potential factors causing an
overall effect. Each cause or reason for imperfection is a source of variation.
Causes are usually grouped into major categories to identify these sources of
variation. The categories typically include:

* People: Anyone involved with the process
* Methods: How the process is performed and the specific requirements for doing
it, such as policies, procedures, rules, regulations and laws
* Machines: Any equipment, computers, tools, etc. required to accomplish the job
* Materials: Raw materials, parts, pens, paper, etc. used to produce the final
product
* Measurements: Data generated from the process that are used to evaluate its
quality
* Environment: The conditions, such as location, time, temperature, and culture
in which the process operates

A typical, simplistic example of an Ishikawa diagram is presented below:

![](images/FishBoneDiagram.gif "Basketball Free-Throws Fish Bone Diagram")

There have been identified topics for fishbone diagram cause classes in
different areas of development. They are grouped into easily remembered sets,
examples of which are presented below.

* The 5 M's (used in manufacturing industry):
    - Machine (technology)
    - Method (process)
    - Material (Includes Raw Material, Consumables and Information)
    - Man/mind Power (physical work)
    - Measurement (Inspection)

* The 8 P's (primarily used in service marketing industry):
    - Product/Service
    - Price
    - Place
    - Promotion
    - People/personnel
    - Process
    - Physical Evidence
    - Publicity

* The 4 S's (used in service industry):
    - Surroundings
    - Suppliers
    - Systems
    - Skills




Cause Screening
---------------

Screening can be used to eliminate brainstorming and hypothesizing at the beginning
of a project. The project team first makes non-invasive observations of the
operation. They ignore everything in the middle, and only compare examples of
the very best and very worst outputs, searching for consistent differences.

The guidelines are simple:

* Any factor that is consistently different when the best and
worst outputs occur is deemed critical, and the team pursues it.
* Any factor that is not consistently different is deemed non-critical,
and the team ignores it.

With this screening process, practitioners non-invasively observe and review
data from the existing operation, and are usually able to separate the critical
and non-critical factors more quickly than with the traditional trial-and-error
approach.

Teamwork
--------

Teamwork should encompass the following:

* Knowledge
* Design
* Redesign

Teamwork is someting that requires training. Product owners should not delegate and
control every aspect, as this can stifle teamwork and involvement. Constant improvement
is the responsibility of managers. Most causes of low quality and productivity are due to
issues at this level. An internal consultant can be hired to reslove teamwork issues.

Aspects related to Teamwork involve:

* Collaboration
* Communication, (intra or inter departmental)
* Involvement
* Training teamwork, along with tools and techniques of quality control, and
philosophy of quality culture

![](images/teamwork-1.png "Kondo Pillars")

Multi-voting
------------

Allow a group to narrow down a list of options into a manageable size for
further consideration. Useful for initiating a selection process after
brainstorming. Multivoting is a group decision-making technique used to reduce a long list of items to a manageable number by means of a structured series of votes. The result is a short list identifying what is important to the team.

Sometimes referred to as `N/3` voting - for N options, each member of the group
selects `N/3` of the options as a means for partial-ordering the options by
importance.

Multivoting Procedures
* Step 1 - Work from a large list
* Step 2 - Assign letter to each item
* Step 3 - Vote
* Step 4 - Tally the votes
* Step 5 - Repeat

Multivoting Rule of Thumb
* Number on Team - Eliminate items with
* 5 or fewer  - 0, 1, or 2 votes
* 6 to 15  - 3 or fewer votes
* more than 15 - 4 or fewer votes

Statistical Process Control
---------------------------

Initially developed during the second world war. Uses a process of (physical)
inspection to separate good products from bad.

* Developing control charts: a production process is in statistical control if
the chart's measurements vary (randomly in some distribution) within the control
limits
* Accepting sampling methods

The achievement of quality should not be considered as separate from the
achievement of production.

Identify product that is beyond reasonable quality control by many (3?) standard
deviations. This product has been produced by unusual circumstances - and that
should be traced and eliminated.

Argument: quality is conformance to requirement and can only be measured by the
cost of non-conformance.

Taguchi
-------

<center>test</center>

Taguchi defines quality as the loss imparted by the product to society from the
time the product is shipped. Quality loss occurs when a product deviates from
target or nominal values.

Statistical methods developed to improve the quality of manufactures goods.
Three principal contributions to statistics:

1. Specific loss function `L(x) = k(x-N)2`
2. The philosophy of off-line quality control
3. Innovation in the design of experiments

Taguchi believed the best opportunity to eliminate variation is during the
design of a product and its manufacturing process. Therefore he developed a
strategy for quality engineering. There are 3 stages:

1. System Design - Conceptual level
2. Parameter (measure) design - nominal values of the various dimensions and
design parameters need to be set. Robustification.
3. Tolerance design - With a successfully completed parameter design, and an
understanding of the effect that the various parameters have on performance,
resources can be focused on reducing and controlling variation in the critical
few dimensions.

TAGUCHI ON QUALITY: Quality has been defined by many as; "being within
specifications," "zero defects," or "customer satisfaction." However, these
definitions do not offer a method of obtaining quality or a means of relating
quality to cost. Taguchi proposes a holistic view of quality which relates
quality to cost, not just to the manufacturer at the time of production, but to
the customer and society as a whole (Phadke, 1989). Taguchi defines quality as,
"The quality of a product is the (minimum) loss imparted by the product to the
society from the time product is shipped" (Bryne and Taguchi, 1986). This
economic loss is associated with losses due to rework, waste of resources during
manufacture, warranty costs, customer complaints and dissatisfaction, time and
money spent by customers on failing products, and eventual loss of market share.

Figure-1 illustrates the loss function and how it relates to the specification
limits. Presented at the 1991 Annual Conference of the International Society of
Parametric Analysts. - 2 - LSL USL LOSS $ TARGET Figure-1; The Quadratic Loss
Function When a critical quality characteristic deviates from the target value,
it causes a loss. In other words, variation from target is the antithesis of
quality. Quality simply means no variability or very little variation from
target performance (Di Lorenzo, 1990). An examination of the loss function shows
that variability reduction or quality improvement drives cost down. Lowest cost
can only be achieved at zero variability from target. Continuously pursuing
variability reduction from the target value in critical quality characteristics
is the key to achieve high quality and reduce cost. Taguchi's quadratic loss
function is the first operational joining of cost of quality and variability of
product that allows design engineers to actually calculate the optimum design
based on cost analysis and experimentation with the design (Teicholz, 1987).

Confirmation is a crucial notion when using Taguchi. Confirmation runs are needed
in order to validate analysis success or identify problems with the experiment.

Advantages
* Orthogonal arrays reduce number of exprimental runs
* Simple calculations
* Enabled effect of control and noise factor variation to be optimised

Limitations
* Does not deal well with interactions
* Not suited to multi-characteristic optimisation
* Large number of runs when using noise arrays

Design of Experiments
---------------------

Design of Experiments (DoE) represents a systematic method to determine the
relationship between factors and their interactions to assess how these are
affecting the output of that process. It is essentially an approach to
determining 'cause and effect' relationships. The DoE method is used to maximize the information obtained from experimental data
through very few trials or runs. Four approaches to DoE are listed below:

1. Full factorial
2. Fractional factorial
3. Taguchi Methods
4. RSM (Response Surface Methodology)

There are typically three components which comprise experimental design. These
are detailed below. We take a simplistic example of a cake-baking process to
contextualise the DoE approach:

* **Factors** represent the inputs to the process: These can be stipulated as either controllable or uncontrollable variables. In the cake-baking process example these would correspond to the oven and ingredients typically.
* **Levels** represent the settings of each factors. Typical examples of this in the cake-baking process would be the oven temperature and the ingredient amounts included.
* **Response** represent the output of the experiment. In the simplistic case of baking a cake these responses could manifest themselves in the form of the taste, consistency and the appearance of the cake. These responses are ultimately affected by the level settings of each factor.

The figure below presents the typical DoE approach to baking a cake,
highlighting the factors, levels and typical responses.


![](images/DesignOfExperiments.png "Design of Experiments Cake Baking Process")


The rationale supporting the cake baking DoE approach is as follows:

* **Comparing ALternatives** - We may want to compare the results from two different types of flour, if it turned out that flour types caused indifferent values, we could purchase from the low cost supplier.
* **Significant Control Factors** - We may want to determine what the significant factors are.
* **Optimal Process Output** - Enables determination of the optimal set of factors and corresponding level  to achieve the exact taste and consistency.
* **Reducing Variability** - Can the recipe be changed slightly without detriment to the responses.
* **Improving Process Robustness** - Improving the fitness for use under varying conditions. For example, can the factors and their levels (recipe) be modified such that cake will come out nearly the same, irrespective of what oven is used.

The pertinent benefits of the DoE approach are as follows:

1. Quick, economical and efficient method to identify most significant input
factors
2. Simultaneous trials with multiple control factors instead of one variable at a
time.
3. Gives interaction effects of control factors / input factors.
4. Less number of trials required to get process insight.
5. Impact of control factors on response can be easily measured.
6. Explore relationship of response and control factors.
7. Cheaper than reducing fabrication or simulation bottlenecks.

Software Process Improvement Tools
----------------------------------

A Software process is defined as the system of all tasks and the supporting
tools, standards, methods, and practices involved in the production and evolution
of a software product. TQM tools, such as Cause & Effect Diagrams, Pareto Charts and
Check Sheets can all be used to improve software processes.

One approach that can be used to improve a business' software process is the PDCA
cycle - Plan, Do, Check and Act (The role of Software Process Improvement into Total
Quality Management: an Industrial Experience, 2000). This approach can be split up into
the following four steps:

* Planning: involves the reviewing of data and information, establishing and documenting
action plans and their deployment in their specific areas
* Do: involves the execution of the plans
* Check: involves performing reviews in their specific areas, integrating leadership and lower
level departments
* Act: involves implementing any system changes and the fulfillment of the goals, following
the reviews.


References
----------

* Total Quality Management: Text, Cases, and Readings, Third Edition; Joel E.
Ross, Susan Perry, 1999
* Fundamentals of Total Quality Management; Jens J. Dahlgaard, Kai Kristensen,
Gopal K. Kanji, 1998
* Total Quality Management as Competitive Advantage: A Review and Empirical
Study; Thomas C. Powell, 1995
* Total quality management implementation frameworks: Comparison and review;
Sha'Ri Mohd Yusof, Elaine Aspinwall, 2000
* Multivoting; University of Kentucky, Program and Staff Development,
<https://psd.ca.uky.edu/files/multivot.pdf>
* Multi-Voting, goLeanSixSigma, <https://goleansixsigma.com/multi-voting/>
* Taguchi Slides, <http://www.slideshare.net/MentariPagi4/tqm-taguchi>
* Taguchi, Teicholz,
<http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20040121019.pdf>
* TQM Software Tools,
<http://www.emeraldinsight.com/doi/pdfplus/10.1108/09544789610125333>
* DoE barriers,
<http://www.emeraldinsight.com/doi/pdfplus/10.1108/17542730910995846>
* ISO 9000 Case Studies,
<http://www.bsigroup.com/en-GB/iso-9001-quality-management/case-studies>
* The role of software process improvement into total quality management: an industrial experience;
R. L. Della Volpe, 2000